{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **P1: Data Processing Problem**\n",
    "This dataset captures key healthcare information, including patient demographics, medical conditions, treatments, and billing. Each record represents a patient’s admission, providing insights into diagnoses, treatments, and outcomes. PySpark's distributed computing capabilities enable efficient analysis, making it ideal for uncovering patterns and trends in healthcare data.\n",
    "\n",
    "\n",
    "### **Dataset Description**\n",
    "\n",
    "| **Column Name**      | **Description**                                              |\n",
    "|-----------------------|--------------------------------------------------------------|\n",
    "| **Name**             | Patient’s name.                                              |\n",
    "| **Age**              | Age of the patient at admission.                             |\n",
    "| **Gender**           | Patient’s gender (Male/Female).                              |\n",
    "| **Blood Type**       | Patient’s blood type (e.g., A+, O-).                         |\n",
    "| **Medical Condition**| Primary diagnosis or health issue.                           |\n",
    "| **Date of Admission**| Date of hospital admission.                                  |\n",
    "| **Doctor**           | Treating doctor’s name.                                      |\n",
    "| **Hospital**         | Name of the healthcare facility.                             |\n",
    "| **Insurance Provider**| Patient’s insurance company.                                |\n",
    "| **Billing Amount**   | Total billed amount for services.                            |\n",
    "| **Room Number**      | Room assigned to the patient.                                |\n",
    "| **Admission Type**   | Type of admission: Emergency, Urgent, or Elective.           |\n",
    "| **Discharge Date**   | Date of discharge from the facility.                         |\n",
    "| **Medication**       | Medication prescribed/administered.                          |\n",
    "| **Test Results**     | Diagnostic test outcomes: Normal, Abnormal, or Inconclusive. |\n",
    "\n",
    "### **The Problem**\n",
    "The goal is to analyze this dataset to uncover insights that improve hospital performance and patient care. PySpark facilitates efficient processing of large-scale data, enabling us to address these key queries:\n",
    "\n",
    "1. **Hospital Performance Evaluation**\n",
    "   - Compare hospitals based on billing, admissions, and a custom KPI.\n",
    "   - Identify hospitals with high emergency cases for resource allocation.\n",
    "\n",
    "2. **Seasonal Trends**\n",
    "   - Analyze seasonal variations in admissions and medical conditions.\n",
    "   - Predict peak periods for emergency care.\n",
    "\n",
    "3. **Patient Demographics Analysis**\n",
    "   - Study medical conditions by gender and age group.\n",
    "   - Explore insurance trends and their impact on billing and outcomes.\n",
    "\n",
    "4. **Length of Stay and Admission Trends**\n",
    "   - Identify conditions associated with extended stays.\n",
    "   - Calculate daily admissions by type for capacity planning.\n",
    "\n",
    "5. **Emergency Admission Trends**\n",
    "   - Detect busiest days for emergency admissions.\n",
    "   - Analyze weekly admission patterns to optimize resource scheduling.\n",
    "\n",
    "6. **Patient Clustering**\n",
    "   - Group patients based on similarities in demographics, medical conditions, and billing.\n",
    "   \n",
    "7. **Doctor Clustering**\n",
    "   - Group doctors based on their specialties, patient outcomes, and billing patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **P2: Leveraging PySpark to address the challenge**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*the dataset was cleaned and checked prior to data extraction, the cleaning procedure was omitted to comply with the assignment requirements. The full procedure is included in the notebook uploaded on our group's folder.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **1. Cross-Hospital Comparison**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This analysis evaluated hospitals using multiple Key Performance Indicators (KPIs) to assess their performance, workload distribution, and patient care profiles.\n",
    "\n",
    "**Steps and PySpark Operations**\n",
    "\n",
    "1. **Metric Calculation**:\n",
    "   - **Average Billing**: Calculated the mean billing amount per hospital using:\n",
    "     ```python\n",
    "     average_billing = df.groupBy(\"Hospital\").agg(avg(\"Billing Amount\").alias(\"Average Billing\"))\n",
    "     ```\n",
    "   - **Average Stay Duration**: Computed the average stay duration for each hospital:\n",
    "     ```python\n",
    "     df = df.withColumn(\"Stay Duration\", datediff(col(\"Discharge Date\"), col(\"Date of Admission\")))\n",
    "     average_stay_duration = df.groupBy(\"Hospital\").agg(avg(\"Stay Duration\").alias(\"Average Stay Duration\"))\n",
    "     ```\n",
    "   - **Admission Type Distribution**: Counted the number of admissions per type (Emergency, Elective, Urgent) for each hospital using:\n",
    "     ```python\n",
    "     admission_type_distribution = df.groupBy(\"Hospital\", \"Admission Type\").count().groupBy(\"Hospital\") \\\n",
    "         .pivot(\"Admission Type\", [\"Emergency\", \"Elective\", \"Urgent\"]).sum(\"count\").na.fill(0)\n",
    "     ```\n",
    "\n",
    "2. **Normalization**:\n",
    "   - Scaled the calculated metrics to make them comparable by dividing each metric by its maximum value\n",
    "\n",
    "3. **KPI Index**:\n",
    "   - Computed a weighted KPI Index to rank hospitals:\n",
    "    \n",
    "4. **Ranking**:\n",
    "   - Ranked hospitals by descending KPI Index to identify the top-performing ones:\n",
    "     ```python\n",
    "     hospital_ranked = hospital_kpis_with_index.orderBy(col(\"KPI Index\").desc())\n",
    "     ```\n",
    "**Results**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre style=\"font-size:12px; color:black;\">\n",
    "\n",
    "Top 10 best hospitals based on KPI index:\n",
    "+--------------------+------------------+---------------------+---------+--------+------+------------------+\n",
    "|            Hospital|   Average Billing|Average Stay Duration|Emergency|Elective|Urgent|         KPI Index|\n",
    "+--------------------+------------------+---------------------+---------+--------+------+------------------+\n",
    "|    Hernandez-Morton|52373.032374241826|                 14.0|        0|       0|     2| 20953.51294969673|\n",
    "|       Walker-Garcia| 52170.03685355641|                  2.0|        0|       0|     2|20868.714741422562|\n",
    "|        Ruiz-Anthony|52154.237721878235|                 23.0|        0|       0|     1|20868.645088751295|\n",
    "|     George-Gonzalez| 52102.24088919256|                  9.0|        1|       0|     0|20843.796355677026|\n",
    "|        Rocha-Carter|52092.669895844054|                  2.0|        1|       0|     0| 20837.86795833762|\n",
    "|Briggs Walker Mar...| 52024.72644288463|                 26.0|        0|       1|     0|20817.740577153854|\n",
    "|and Small Stephen...| 51975.96813526631|                 27.0|        0|       0|     2|20798.587254106522|\n",
    "|      Clark-Espinoza| 51848.20159668146|                  8.0|        0|       0|     1|20741.730638672587|\n",
    "|        Stephens Ltd| 51714.30087099009|                 27.0|        0|       0|     1|20693.870348396034|\n",
    "|Pierce and Miller...| 51722.12273936527|                  7.0|        0|       1|     0| 20690.99909574611|\n",
    "+--------------------+------------------+---------------------+---------+--------+------+------------------+\n",
    "only showing top 10 rows\n",
    "\n",
    "--> queries execution started at 01:02:58.884276 and ended at 01:03:00.845048, execution time: 1960ms\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **2. Seasonal Trends**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This analysis explored seasonal variations in hospital admissions, identifying trends in patient visits and the most common medical conditions during each season.    \n",
    "Additionally, the season with the highest number of admissions was determined.\n",
    "\n",
    "**Steps and PySpark Operations**\n",
    "\n",
    "1. **Adding a \"Season\" Column**:\n",
    "   - A new column, `Season`, was added based on the month of the `Date of Admission`. The months were mapped as follows:\n",
    "     ```python\n",
    "     df = df.withColumn(\n",
    "         \"Season\",\n",
    "         when(month(col(\"Date of Admission\")).isin(3, 4, 5), \"Spring\")\n",
    "         .when(month(col(\"Date of Admission\")).isin(6, 7, 8), \"Summer\")\n",
    "         .when(month(col(\"Date of Admission\")).isin(9, 10, 11), \"Autumn\")\n",
    "         .when(month(col(\"Date of Admission\")).isin(12, 1, 2), \"Winter\"))\n",
    "     ```\n",
    "2. **Grouping by Season and Medical Condition**:\n",
    "   - Admissions were grouped by `Season` and `Medical Condition`, and the count of admissions was calculated:\n",
    "     ```python\n",
    "     seasonal_counts = df.groupBy(\"Season\", \"Medical Condition\").agg(\n",
    "         count(\"*\").alias(\"Admission Count\"))\n",
    "     ```\n",
    "3. **Identifying the Most Common Medical Condition per Season**:\n",
    "   - A **window function** was used to rank medical conditions by admission count within each season:\n",
    "     ```python\n",
    "     window_spec = Window.partitionBy(\"Season\").orderBy(col(\"Admission Count\").desc())\n",
    "     most_common_conditions = seasonal_counts.withColumn(\n",
    "         \"Rank\", row_number().over(window_spec)).filter(col(\"Rank\") == 1)\n",
    "     ```\n",
    "4. **Finding the Season with the Most Admissions**:\n",
    "   - Admissions were grouped by `Season`, and the total admissions for each season were calculated:\n",
    "     ```python\n",
    "     seasonal_totals = df.groupBy(\"Season\").agg(\n",
    "         count(\"*\").alias(\"Total Admissions\"))\n",
    "     ```\n",
    "   - The season with the highest number of admissions was identified using:\n",
    "     ```python\n",
    "     most_admissions_season = seasonal_totals.orderBy(col(\"Total Admissions\").desc())\n",
    "     ```\n",
    " **Results**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"display: flex; gap: 10px;\">\n",
    "\n",
    "  <div style=\"width: 35%; font-size:12px; color:black;\">\n",
    "    <pre>\n",
    "Most Common Medical Conditions per Season:\n",
    "+------+-----------------+---------------+\n",
    "|Season|Medical Condition|Admission Count|\n",
    "+------+-----------------+---------------+\n",
    "|Autumn|         Diabetes|           2346|\n",
    "|Spring|           Cancer|           2335|\n",
    "|Summer|        Arthritis|           2447|\n",
    "|Winter|        Arthritis|           2348|\n",
    "+------+-----------------+---------------+\n",
    "    </pre>\n",
    "  </div>\n",
    "  \n",
    "  <div style=\"width: 25%; font-size:12px; color:black;\">\n",
    "    <pre>\n",
    "Season with the Most Admissions:\n",
    "+------+----------------+\n",
    "|Season|Total Admissions|\n",
    "+------+----------------+\n",
    "|Summer|           14343|\n",
    "|Spring|           13789|\n",
    "|Autumn|           13772|\n",
    "|Winter|           13596|\n",
    "+------+----------------+\n",
    "    </pre>\n",
    "  </div>\n",
    "\n",
    "   <div style=\"width: 20%; font-size:12px; color:black;\">\n",
    "    <pre>\n",
    "\n",
    "Season with the Most Admissions:\n",
    "+------+----------------+\n",
    "|Season|Total Admissions|\n",
    "+------+----------------+\n",
    "|Summer|           14343|\n",
    "+------+----------------+\n",
    "    </pre>\n",
    "  </div>\n",
    "\n",
    "</div>\n",
    "\n",
    "<pre style=\"font-size:12px; color:black;\">\n",
    "--> queries execution started at 01:05:29.777919 and ended at 01:05:30.217527, execution time: 439ms\n",
    "</pre>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **3. Patient Demographics Analysis** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This analysis examined the distribution of patients based on their medical conditions, age groups, and gender. Various charts were used to visualize these relationships and provide actionable insights.\n",
    "\n",
    "**Steps and PySpark Operations**\n",
    "\n",
    "1. **Counting Patients by Medical Condition**:\n",
    "   - Grouped the dataset by `Medical Condition` and counted the number of patients for each condition using:\n",
    "     ```python\n",
    "     patients_by_condition = df.groupBy(\"Medical Condition\").agg(count(\"ID\").alias(\"Patients\"))\n",
    "     ```\n",
    "   - The data was visualized in a **pie chart** to display the proportion of patients by condition.\n",
    "\n",
    "2. **Counting Patients by Medical Condition and Age**:\n",
    "   - Grouped by `Medical Condition` and `Age` to calculate the number of patients for each medical condition in each age group:\n",
    "     ```python\n",
    "     medical_condition_per_age = df.groupBy(\"Medical Condition\", \"Age\").agg(\n",
    "         count(\"ID\").alias(\"Patients\")\n",
    "     ).orderBy(\"Age\", ascending=True)\n",
    "     ```\n",
    "3. **Ranking and Filtering Top Conditions by Age Group**:\n",
    "   - Applied a **window function** to rank medical conditions within each age group by patient count:\n",
    "     ```python\n",
    "     window_spec = Window.partitionBy(\"Age\").orderBy(col(\"Patients\").desc())\n",
    "     ranked_conditions = medical_condition_per_age.withColumn(\"Rank\", row_number().over(window_spec))\n",
    "     ```\n",
    "   - Filtered to retrieve the top-ranked condition for each age group:\n",
    "     ```python\n",
    "     top_conditions_by_age_group = ranked_conditions.filter(col(\"Rank\") == 1)\n",
    "     ```\n",
    "4. **Counting Patients by Gender and Medical Condition**:\n",
    "   - Grouped the dataset by `Gender` and `Medical Condition` and calculated the patient count for each combination:\n",
    "     ```python\n",
    "     gender_condition = df.groupBy(\"Gender\", \"Medical Condition\").agg(count(\"ID\").alias(\"Patients\"))\n",
    "     ```\n",
    "   - Data was visualized in a **bar chart** to compare male and female patient counts for each condition.\n",
    "\n",
    "**Results**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"display: flex; gap: 5px;\">\n",
    "\n",
    "  <div style=\"width: 40%; font-size:12px; color:black;\">\n",
    "    <pre>\n",
    "Gender and Medical Conditions:\n",
    "+------+-----------------+--------+\n",
    "|Gender|Medical Condition|Patients|\n",
    "+------+-----------------+--------+\n",
    "|Female|        Arthritis|    4686|\n",
    "|Female|     Hypertension|    4612|\n",
    "|Female|           Cancer|    4602|\n",
    "|Female|         Diabetes|    4651|\n",
    "|Female|          Obesity|    4622|\n",
    "|Female|           Asthma|    4553|\n",
    "|  Male|     Hypertension|    4633|\n",
    "|  Male|           Asthma|    4632|\n",
    "|  Male|        Arthritis|    4622|\n",
    "|  Male|          Obesity|    4609|\n",
    "|  Male|         Diabetes|    4653|\n",
    "|  Male|           Cancer|    4625|\n",
    "+------+-----------------+--------+\n",
    "    </pre>\n",
    "  </div>\n",
    "  \n",
    "  <div style=\"width: 45%; font-size:12px; color:black;\">\n",
    "    <pre>\n",
    "Medical conditions and their distribution:\n",
    "+-----------------+--------+\n",
    "|Medical Condition|Patients|\n",
    "+-----------------+--------+\n",
    "|          Obesity|    9231|\n",
    "|         Diabetes|    9304|\n",
    "|        Arthritis|    9308|\n",
    "|     Hypertension|    9245|\n",
    "|           Cancer|    9227|\n",
    "|           Asthma|    9185|\n",
    "+-----------------+--------+\n",
    "</pre>\n",
    "  </div>\n",
    "\n",
    "</div>\n",
    "\n",
    "<pre style=\"font-size:12px; color:10px;\">\n",
    "--> queries execution started at 01:05:30.324017 and ended at 01:05:31.735653, execution time: 1411ms\n",
    "</pre>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **4. Analyzing Patients Length of Stay and Admission Trends**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This analysis identified the medical condition-admission type combinations with the highest total length of stay and calculated average daily admissions for each admission type, providing insights for hospital resource management.\n",
    "\n",
    "**Steps and PySpark Operations**\n",
    "1. **Converting Dates**:\n",
    "   - Converted `Date of Admission` and `Discharge Date` to `date` type for accurate calculations using:\n",
    "     ```python\n",
    "     df.withColumn(\"Date of Admission\", to_date(\"Date of Admission\", \"yyyy-MM-dd\"))\n",
    "     df.withColumn(\"Discharge Date\", to_date(\"Discharge Date\", \"yyyy-MM-dd\"))\n",
    "     ```\n",
    "2. **Grouping by Medical Condition and Admission Type**:\n",
    "   - Used `groupBy` and `agg` to calculate:\n",
    "     - `count(\"*\")`: Total patient count.\n",
    "     - `sum(\"Stay Duration\")`: Total length of stay.\n",
    "     - `avg(\"Stay Duration\")`: Average length of stay.\n",
    "     ```python\n",
    "     group_stats = df.groupBy(\"Medical Condition\", \"Admission Type\").agg(\n",
    "         count(\"*\").alias(\"Patient_Count\"),\n",
    "         sum(\"Stay Duration\").alias(\"Total Length of Stay\"),\n",
    "         avg(\"Stay Duration\").alias(\"Avg Length of Stay\"))\n",
    "     ```\n",
    "3. **Top 5 Groups by Total Length of Stay**:\n",
    "   - Ordered groups in descending order of total stay duration using `orderBy` and retrieved the top 5:\n",
    "     ```python\n",
    "     top_5_groups = group_stats.orderBy(desc(\"Total Length of Stay\")).limit(5)\n",
    "     ```\n",
    "4. **Calculating Average Daily Admissions**:\n",
    "   - Grouped by `Admission Type` and `Date of Admission` to count daily admissions, then averaged these counts:\n",
    "     ```python\n",
    "     daily_admissions = df.groupBy(\"Admission Type\", \"Date of Admission\").count() \\\n",
    "                          .groupBy(\"Admission Type\").agg(\n",
    "                              avg(\"count\").alias(\"Avg_Daily_Admissions\"))\n",
    "     ```\n",
    "**Results**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div style=\"display: flex; gap: 20px;\">\n",
    "\n",
    "  <div style=\"width: 55%; font-size:10px; color:black;\">\n",
    "    <pre>\n",
    "Top 5 groups with the highest total length of stay:\n",
    "+-----------------+--------------+-------------+--------------------+------------------+\n",
    "|Medical Condition|Admission Type|Patient_Count|Total Length of Stay|Avg Length of Stay|\n",
    "+-----------------+--------------+-------------+--------------------+------------------+\n",
    "|     Hypertension|      Elective|         3221|               49843|15.474386836386216|\n",
    "|           Asthma|      Elective|         3102|               48836|15.743391360412637|\n",
    "|         Diabetes|        Urgent|         3229|               48729|15.091049860637968|\n",
    "|           Asthma|        Urgent|         3081|               48573|15.765335929892892|\n",
    "|          Obesity|     Emergency|         3126|               48508|15.517594369801664|\n",
    "+-----------------+--------------+-------------+--------------------+------------------+\n",
    "    </pre>\n",
    "  </div>\n",
    "  \n",
    "  <div style=\"width: 45%; font-size:10px; color:black;\">\n",
    "    <pre>\n",
    "\n",
    "Average daily admissions per admission type:\n",
    "+--------------+--------------------+\n",
    "|Admission Type|Avg_Daily_Admissions|\n",
    "+--------------+--------------------+\n",
    "|      Elective|  10.210727969348659|\n",
    "|     Emergency|   9.999452654625069|\n",
    "|        Urgent|  10.173055859802847|\n",
    "+--------------+--------------------+\n",
    "    </pre>\n",
    "  </div>\n",
    "\n",
    "</div>\n",
    "\n",
    "<pre style=\"font-size:12px; color:black;\">\n",
    "--> queries execution started at 01:05:31.822585 and ended at 01:05:32.089082, execution time: 266ms\n",
    "</pre>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **5. Emergency Admission Trends** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis emergency room data to identify the busiest days for emergency admissions, with the study of weekly admission distribution.\n",
    "\n",
    "**Steps and PySpark Operations**\n",
    "1. **Filtering Emergency Cases**:\n",
    "  - Filtered the dataset to include only rows where `Admission Type` is **Emergency**:\n",
    "    ```python\n",
    "    emergency_cases = df.filter(col(\"Admission Type\") == \"Emergency\")\n",
    "    ```\n",
    "2. **Extracting the Day of the Week**:\n",
    "  - Added a new column, `Day_of_Week`, by extracting the day of the week from the `Date of Admission`:\n",
    "    ```python\n",
    "    emergency_cases = emergency_cases.withColumn(\"Day_of_Week\", date_format(\"Date of Admission\", \"E\"))\n",
    "    ```\n",
    "3. **Counting Admissions by Day**:\n",
    "  - Grouped by `Day_of_Week` and counted the number of emergency admissions for each day:\n",
    "    ```python\n",
    "    admissions_by_day = emergency_cases.groupBy(\"Day_of_Week\").count().orderBy(desc(\"count\"))\n",
    "    ```\n",
    "**Results**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div style=\"display: flex; gap: 1px;\">\n",
    "\n",
    "  <div style=\"width: 30%; font-size:12px; color:black;\">\n",
    "    <pre>\n",
    "\n",
    "Admissions distribution over the week:\n",
    "+-----------+-----+\n",
    "|Day_of_Week|count|\n",
    "+-----------+-----+\n",
    "|        Tue| 2645|\n",
    "|        Fri| 2628|\n",
    "|        Thu| 2609|\n",
    "|        Sun| 2602|\n",
    "|        Mon| 2597|\n",
    "|        Wed| 2597|\n",
    "|        Sat| 2591|\n",
    "+-----------+-----+\n",
    "    </pre>\n",
    "  </div>\n",
    "  \n",
    "  <div style=\"width: 45%; font-size:12px; color:black;\">\n",
    "    <pre>\n",
    "  Top 2 Busiest Days for Emergency Admissions:\n",
    "  +-----------+-----+\n",
    "  |Day_of_Week|count|\n",
    "  +-----------+-----+\n",
    "  |        Tue| 2645|\n",
    "  |        Fri| 2628|\n",
    "  +-----------+-----+\n",
    "    </pre>\n",
    "  </div>\n",
    "\n",
    "</div>\n",
    "\n",
    "<pre style=\"font-size:12px; color:black;\">\n",
    "--> queries execution started at 01:05:31.822585 and ended at 01:05:32.089082, execution time: 266ms\n",
    "</pre>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **6. Patient Clustering**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clustering patient admissions to identify patterns in admission criteria and utilize historical data to predict possible medications or treatment plans.  \n",
    "Admissions were treated as independent entities, accounting for potential readmissions.\n",
    "\n",
    "**Steps and PySpark Operations**\n",
    "\n",
    "1. **Data Preparation**:\n",
    "   - Selected relevant columns (`ID`, `Age`, `Stay Duration`, `Medical Condition`, `Billing Amount`) for clustering\n",
    "  \n",
    "   - **Tokenization**: Split `Medical Condition` into tokens using:\n",
    "     ```python\n",
    "     tokenizer = Tokenizer(inputCol=\"Medical Condition\", outputCol=\"Medical Condition Tokens\")\n",
    "     ```\n",
    "   - **Hashing**: Converted medical condition tokens into numerical features:\n",
    "     ```python\n",
    "     hashingTF = HashingTF(inputCol=\"Medical Condition Tokens\", outputCol=\"Medical Condition Hashed\", numFeatures=42)\n",
    "     ```\n",
    "2. **Feature Engineering**:\n",
    "   - Combined features (`Age`, `Stay Duration`, `Billing Amount`, `Medical Condition Hashed`) using:\n",
    "     ```python\n",
    "     assembler = VectorAssembler( inputCols=[\"Age\", \"Stay Duration\", \"Billing Amount\", \"Medical Condition Hashed\"], outputCol=\"features\")\n",
    "     ```\n",
    "   - Scaled the features for normalization using:\n",
    "     ```python\n",
    "     scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\", withStd=True, withMean=False)\n",
    "     ```\n",
    "3. **Finding Optimal Number of Clusters (k)**:\n",
    "   - Best k was evaluated via **Silhouette Score** for `k` values between 2 and 10\n",
    "\n",
    "4. **Training the KMeans Model**:\n",
    "   - Trained the final KMeans model with the optimal `k` (found to be 6):\n",
    "     ```python\n",
    "     kmeans = KMeans(featuresCol=\"scaled_features\", k=best_k)\n",
    "     model = kmeans.fit(dataset_for_clustering)\n",
    "     patients = model.transform(dataset_for_clustering)\n",
    "     ```\n",
    "5. **Cluster Statistics**:\n",
    "   - Grouped patients by their clusters to compute aggregate statistics (e.g., average age, stay duration, billing amount), check the output below.\n",
    "\n",
    "**Insights**\n",
    "- Clustering revealed patterns in admissions, such as similarities in patient demographics, medical conditions, and financial data.\n",
    "- PySpark’s tools like **Tokenizer**, **HashingTF**, **VectorAssembler**, and **KMeans** efficiently processed and modeled the data at scale.\n",
    "\n",
    "**Results**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre style=\"font-size:8px; color:black;\">\n",
    "Obtained clusters:\n",
    "+----------+------------------+------------------+-----------+-----------+---------------------+---------------------+---------------------+----------------------+----------------------+----------------------+\n",
    "|prediction|Number of Patients|Average Age       |Minimum Age|Maximum Age|Average Stay Duration|Minimum Stay Duration|Maximum Stay Duration|Average Billing Amount|Minimum Billing Amount|Maximum Billing Amount|\n",
    "+----------+------------------+------------------+-----------+-----------+---------------------+---------------------+---------------------+----------------------+----------------------+----------------------+\n",
    "|1         |9304              |51.55417024935512 |14         |89         |15.422936371453138   |1                    |30                   |25640.13433762909     |31.030955450754846    |52211.85296638021     |\n",
    "|3         |9308              |51.56532015470563 |13         |89         |15.517404383326172   |1                    |30                   |25498.583437499496    |26.112523116277544    |52170.03685355641     |\n",
    "|5         |9231              |51.240277326400175|13         |89         |15.464305059040191   |1                    |30                   |25808.21690005182     |36.217270349314276    |52024.72644288463     |\n",
    "|4         |9245              |51.741914548404544|13         |89         |15.458626284478097   |1                    |30                   |25498.991212821315    |23.866729145456247    |52764.276736469175    |\n",
    "|2         |9227              |51.558794841226835|13         |89         |15.495827462880676   |1                    |30                   |25164.17765586996     |9.238787497393332     |52373.032374241826    |\n",
    "|0         |9185              |51.575830157866086|13         |89         |15.69657049537289    |1                    |30                   |25637.40628768246     |32.34872934198097     |52181.837792399056    |\n",
    "+----------+------------------+------------------+-----------+-----------+---------------------+---------------------+---------------------+----------------------+----------------------+----------------------+\n",
    "\n",
    "--> queries execution started at 02:10:43.734777 and ended at 02:10:55.449092, execution time: 11714ms\n",
    "</pre>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **7. Doctor Clustering**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this analysis, we clustered doctors based on their performance metrics and patient profiles to identify patterns in treatment approaches and specialties.    \n",
    "Although it's a different analysis, the pySpark operations applied do not differ from the ones applied in the clustering of patients.    \n",
    "Let's simply jump to the results    \n",
    "\n",
    "**Results**:\n",
    "<pre style=\"font-size:12px; color:black;\">\n",
    "Obtained clusters:\n",
    "+----------+-----------------+--------------------------+----------------------+---------------------+-------------------------+\n",
    "|prediction|Number of Doctors|Average Number of Patients|Average Billing Amount|Average Stay Duration|Average Unique Conditions|\n",
    "+----------+-----------------+--------------------------+----------------------+---------------------+-------------------------+\n",
    "|1         |3397             |3.587871651457168         |25354.59790957738     |15.603407572963658   |3.587871651457168        |\n",
    "|0         |36944            |1.1723689909051538        |25590.374342654613    |15.488644976180165   |1.1723689909051538       |\n",
    "+----------+-----------------+--------------------------+----------------------+---------------------+-------------------------+\n",
    "\n",
    "--> queries execution started at 02:30:31.987874 and ended at 02:30:53.599090, execution time: 21611ms\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **P3: Was it really worth it?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Cluster Mode vs Local Execution**\n",
    "\n",
    "This final part evaluates the performance of a Spark program when executed in two different environments. The first environment is our university cluster of 10 computers. The second environment involves running the program locally on a single machine, which is tested in the following configurations: single core (`local[1]`), two cores, and using all available core (`local[*]`). \n",
    "\n",
    "**dataset description**: 55000 rows, 15 columns [≈8MB]\n",
    " \n",
    "results:\n",
    "\n",
    "| **Mode**                | **Average Execution Time** (ms) |\n",
    "|--------------------------|--------------------|\n",
    "| `cluster mode`           | ≈ 413017      |\n",
    "| `local[1]`| ≈ 39008       |\n",
    "| `local[2]` | ≈ 31258       |\n",
    "| `local[*]`| ≈ 28624      |                  \n",
    "                               \n",
    "*The reported times represent the average of 10 runs for each configuration*\n",
    "\n",
    "\n",
    "The significant difference in execution times between cluster mode and local execution highlights the impact of workload distribution and system overhead on performance. In cluster mode, Spark distributes the dataset into partitions and sends tasks to multiple worker nodes in the cluster. This process involves overhead such as communication between the driver and workers, data shuffling, and coordination of tasks. For small datasets like the one used in this experiment (8MB), the communication and coordination overhead become the dominant factors, leading to much slower execution compared to local processing. Distributed systems like Spark are designed for large-scale datasets, where the benefits of parallel computation outweigh these overheads. However, for small workloads, such overhead can render cluster mode inefficient.\n",
    "\n",
    "On the other hand, local execution demonstrates a substantial performance advantage due to the absence of such overhead. When running with a single thread (`local[1]`), the program executes each task sequentially. Although this is slower than using multiple threads, it still outperforms cluster mode by avoiding distributed communication. When executed with all available threads (`local[*]`), Spark leverages all CPU cores on the local machine to parallelize the workload, further reducing execution time. This improvement highlights the efficiency of parallel processing at the local level, where Spark can process tasks concurrently without the need for inter-node communication. \n",
    "\n",
    "Interestingly, the difference in execution times between `local[1]` and `local[*]` is not as significant as one might expect for this dataset size. This could be attributed to the relatively small dataset, where the overhead of managing parallel threads is comparable to the benefits gained from parallel execution.\n",
    "\n",
    "summary:\n",
    "\n",
    "| **Execution Environment** | **Performance Impact**                                         |\n",
    "|----------------------------|---------------------------------------------------------------|\n",
    "| **Cluster Mode**           | Slower due to distributed computing overhead and small dataset. |\n",
    "| **Local Mode (1 Thread)**  | Faster than cluster mode but limited by single-thread execution. |\n",
    "| **Local Mode (local[*])**  | Fastest as it utilizes all available CPU cores for parallelism. |\n",
    "\n",
    "This experiment underscores the importance of choosing the appropriate execution environment based on the size of the dataset and the characteristics of the workload. Cluster mode, while powerful for large datasets, introduces significant overhead when processing smaller data, making it a suboptimal choice in this scenario. Local execution, particularly with `local[*]`, proves to be the most efficient option, as it capitalizes on the available computational resources without incurring the overhead of distributed communication. \n",
    "\n",
    "Ultimately this introduces a trade-off where, for small datasets like the one used here, local execution is to be chosen, while cluster mode should be reserved for larger datasets where the benefits of distributed computation justify its overhead.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
